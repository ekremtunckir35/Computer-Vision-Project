{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe9vkEvFABbN"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# How to Train YOLOv8 Object Detection on a Custom Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n",
    "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
    "\n",
    "## âš ï¸ Disclaimer\n",
    "\n",
    "YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
    "\n",
    "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
    "\n",
    "## Accompanying Blog Post\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Object Detection, concurrently.\n",
    "\n",
    "## Pro Tip: Use GPU Acceleration\n",
    "\n",
    "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
    "\n",
    "## Steps in this Tutorial\n",
    "\n",
    "In this tutorial, we are going to cover:\n",
    "\n",
    "- Before you start\n",
    "- Install YOLOv8\n",
    "- CLI Basics\n",
    "- Inference with Pre-trained COCO Model\n",
    "- Roboflow Universe\n",
    "- Preparing a custom dataset\n",
    "- Custom Training\n",
    "- Validate Custom Model\n",
    "- Inference with Custom Model\n",
    "\n",
    "**Let's begin!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyRdDYkqAKN4"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8cDtxLIBHgQ",
    "outputId": "95135e49-c7b1-4115-c4af-18f950485316"
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:52:49.012803Z",
     "start_time": "2024-08-13T15:52:49.007005Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "da87c813-6c0a-4902-c07d-29407ccb5ec4"
   },
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3EO_2zNChu"
   },
   "source": [
    "## Install YOLOv8\n",
    "\n",
    "âš ï¸ YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
    "\n",
    "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
    "\n",
    "YOLOv8 can be installed in two waysâ€Š-â€Šfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:52:56.756558Z",
     "start_time": "2024-08-13T15:52:53.635485Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdSMcABDNKW-",
    "outputId": "002d1741-8bb4-4be5-f489-1e17dc7ea9b1"
   },
   "source": [
    "# Pip install method (recommended)\n",
    "\n",
    "!pip install ultralytics==8.0.196\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVvaIYEEPOty"
   },
   "source": [
    "# Git clone method (for development)\n",
    "\n",
    "# %cd {HOME}\n",
    "# !git clone github.com/ultralytics/ultralytics\n",
    "# %cd {HOME}/ultralytics\n",
    "# !pip install -e .\n",
    "\n",
    "# from IPython import display\n",
    "# display.clear_output()\n",
    "\n",
    "# import ultralytics\n",
    "# ultralytics.checks()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:00.450582Z",
     "start_time": "2024-08-13T15:53:00.447113Z"
    },
    "id": "VOEYrlBoP9-E"
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnnZSm5OQfPQ"
   },
   "source": [
    "## CLI Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K33S7zlkQku0"
   },
   "source": [
    "If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started. Read more about CLI in [Ultralytics YOLO Docs](https://docs.ultralytics.com/usage/cli/).\n",
    "\n",
    "```\n",
    "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
    "          classify       predict        yolov8n-cls.yaml  args...\n",
    "          segment        val            yolov8n-seg.yaml  args...\n",
    "                         export         yolov8n.pt        format=onnx  args...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5RGYA6sPgEd"
   },
   "source": [
    "## Inference with Pre-trained COCO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT1qD4toTTw0"
   },
   "source": [
    "### ğŸ’» CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaE1kLS8R4CV"
   },
   "source": [
    "`yolo mode=predict` runs YOLOv8 inference on a variety of sources, downloading models automatically from the latest YOLOv8 release, and saving results to `runs/predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDbMt_M6PiXb",
    "outputId": "3c2fd973-0d92-4f5c-dbc4-8800c7de87f6"
   },
   "source": [
    "%cd {HOME}\n",
    "!yolo task=detect mode=predict model=yolov8n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "LyopYpK1TQrB",
    "outputId": "287966c3-84c5-4bb4-8163-8911acb4d37a"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename='runs/detect/predict/dog.jpeg', height=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFMBYQtMVL-B"
   },
   "source": [
    "### ğŸ Python SDK\n",
    "\n",
    "The simplest way of simply using YOLOv8 directly in a Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx9NWF-sVN6Y",
    "outputId": "f8b8e341-ae74-4b33-92ea-0e0c10a63902"
   },
   "source": [
    "model = YOLO(f'{HOME}/yolov8n.pt')\n",
    "results = model.predict(source='https://media.roboflow.com/notebooks/examples/dog.jpeg', conf=0.25)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAi4PvrItTCf",
    "outputId": "3a1a1c21-be10-437f-aa14-4995d5321789"
   },
   "source": [
    "results[0].boxes.xyxy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqT2M01K1LUb",
    "outputId": "ac8d0988-8be7-4fec-c62b-2cd8fe9b5371"
   },
   "source": [
    "results[0].boxes.conf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKIwJ5yw1PMb",
    "outputId": "ee27ea55-240f-43fd-d9a3-e8b8a73149fb"
   },
   "source": [
    "results[0].boxes.cls"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Xtaekw3271"
   },
   "source": [
    "## Roboflow Universe\n",
    "\n",
    "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 110,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
    "\n",
    "\n",
    "[![Roboflow Universe](https://media.roboflow.com/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JHICVjZbVKn"
   },
   "source": [
    "## Preparing a customÂ dataset\n",
    "\n",
    "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
    "\n",
    "### Step 1: Creating project\n",
    "\n",
    "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 2: Uploading images\n",
    "\n",
    "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
    "\n",
    "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 3: Labeling\n",
    "\n",
    "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 4: Generate new dataset version\n",
    "\n",
    "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 5: Exporting dataset\n",
    "\n",
    "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:16.466270Z",
     "start_time": "2024-08-13T15:53:12.397419Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSd93ZJzZZKt",
    "outputId": "f5f44c10-3908-4d95-d5db-5862bd825387"
   },
   "source": [
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets\n",
    "\n",
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"0i8v0fEn4mty6Zt83ACV\")\n",
    "project = rf.workspace(\"myworkspace-bkrfh\").project(\"analiz-ssev8\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUjFBKKqXa-u"
   },
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2YkphuiaE7_",
    "outputId": "f989e4d8-f9e5-47ea-96e2-0062e0c0aff5"
   },
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=25 imgsz=800 plots=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kod parÃ§acÄ±ÄŸÄ±, nesne algÄ±lama gÃ¶revi iÃ§in Ã¶zel bir veri kÃ¼mesi Ã¼zerinde bir YOLOv8 modelini eÄŸitmek iÃ§in kullanÄ±lÄ±r. EÄŸitim, 800x800 piksele yeniden boyutlandÄ±rÄ±lmÄ±ÅŸ gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde 25 epok boyunca Ã§alÄ±ÅŸacaktÄ±r. Komut, modelin nasÄ±l Ã¶ÄŸrendiÄŸini gÃ¶rselleÅŸtirmeye yardÄ±mcÄ± olmak iÃ§in eÄŸitim sÄ±rasÄ±nda otomatik olarak grafikler oluÅŸturacak ÅŸekilde ayarlanmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MScstfHhArr",
    "outputId": "05bd1c5d-a582-45f8-cd1b-7accf6a34809"
   },
   "source": [
    "!ls {HOME}/runs/detect/train/"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**args.yaml**: Genellikle eÄŸitim sÃ¼recinde kullanÄ±lan model yapÄ±landÄ±rmasÄ±, Ã¶ÄŸrenme oranÄ±, yÄ±ÄŸÄ±n boyutu vb. gibi argÃ¼manlarÄ± veya hiperparametreleri depolayan bir YAML dosyasÄ±.\n",
    "\n",
    "**confusion_matrix_normalized.png ve confusion_matrix.png**: Bunlar eÄŸitim sÄ±rasÄ±nda oluÅŸturulan karÄ±ÅŸÄ±klÄ±k matrislerini temsil eden gÃ¶rÃ¼ntÃ¼lerdir (PNG formatÄ±nda).\n",
    "\n",
    "**Confusion Matrix:(KarÄ±ÅŸÄ±klÄ±k Matrisi)**: GerÃ§ek ve tahmin edilen sÄ±nÄ±flandÄ±rmalarÄ± gÃ¶stererek bir sÄ±nÄ±flandÄ±rma algoritmasÄ±nÄ±n performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lan bir tablo.\n",
    "\n",
    "**Normalized:** NormalleÅŸtirilmiÅŸ karÄ±ÅŸÄ±klÄ±k matrisi, ham sayÄ±lar yerine tahminlerin oranlarÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**events.out.tfevents:** Bu, eÄŸitim sÄ±rasÄ±nda Ã¶lÃ§Ã¼mleri gÃ¶rselleÅŸtirmek iÃ§in bir araÃ§ olan TensorBoard tarafÄ±ndan oluÅŸturulan bir dosyadÄ±r. Dosya skaler deÄŸerler, histogramlar, gÃ¶rÃ¼ntÃ¼ler ve daha fazlasÄ± gibi olay verilerini iÃ§erir.\n",
    "\n",
    "\n",
    "**F1_curve.png:** EÄŸitim sÄ±rasÄ±nda F1 skoru eÄŸrisini temsil eden bir gÃ¶rÃ¼ntÃ¼.  \n",
    "\n",
    "**F1 Score(F1 PuanÄ±)**: Kesinlik ve geri Ã§aÄŸÄ±rmayÄ± dengeleyen, Ã¶zellikle dengesiz veri kÃ¼melerinde yararlÄ± olan bir metrik.\n",
    "\n",
    "**labels_correlogram.jpg and labels.jpg:** \n",
    "\n",
    "**Labels Correlagram:**(Etiketler KorelagramÄ±): FarklÄ± etiketler veya sÄ±nÄ±flar arasÄ±ndaki korelasyonu gÃ¶steren gÃ¶rsel bir temsil.\n",
    "\n",
    "**Labels.jpg:** BÃ¼yÃ¼k olasÄ±lÄ±kla veri kÃ¼mesinde bulunan etiketlerin gÃ¶rselleÅŸtirilmesi.\n",
    "\n",
    "**P_curve.png, PR_curve.png, R_curve.png:**\n",
    "\n",
    "**P_curve.png** EÄŸitim sÄ±rasÄ±nda hassasiyet eÄŸrisi.\n",
    "\n",
    "**PR_curve.png:** Precision-Recall curve(Hassasiyet-Geri Ã‡aÄŸÄ±rma eÄŸrisi), farklÄ± eÅŸikler iÃ§in hassasiyet ve geri Ã§aÄŸÄ±rma arasÄ±ndaki deÄŸiÅŸ tokuÅŸu gÃ¶steren grafiksel bir gÃ¶sterimdir.\n",
    "\n",
    "**R_curve.png:** EÄŸitim sÄ±rasÄ±nda hatÄ±rlama eÄŸrisi.\n",
    "\n",
    "**results.csv and results.png:** \n",
    "\n",
    "**results.csv:** DoÄŸruluk, kesinlik, geri Ã§aÄŸÄ±rma gibi metrikler de dahil olmak Ã¼zere eÄŸitim sÃ¼recinin ayrÄ±ntÄ±lÄ± sonuÃ§larÄ±nÄ± iÃ§eren bir CSV dosyasÄ±.\n",
    "\n",
    "**results.png:** EÄŸitim sonuÃ§larÄ±nÄ±n gÃ¶rsel bir temsili (muhtemelen bir Ã¶zet).\n",
    "\n",
    "**train_batch0.jpg, train_batch1.jpg, train_batch2.jpg, etc.:**\n",
    "\n",
    "Bunlar eÄŸitim gruplarÄ±ndan gÃ¶rÃ¼ntÃ¼lerdir. Muhtemelen modelin eÄŸitim sÄ±rasÄ±nda bu gÃ¶rÃ¼ntÃ¼lerdeki nesneleri nasÄ±l tahmin ettiÄŸine dair Ã¶rnekler gÃ¶sterirler.\n",
    "\n",
    "**val_batch0_labels.jpg and val_batch0_pred.jpg:**\n",
    "\n",
    "**val_batch0_labels.jpg:** Bir doÄŸrulama grubu iÃ§in temel gerÃ§ek etiketlerini gÃ¶steren bir gÃ¶rÃ¼ntÃ¼.\n",
    "\n",
    "**val_batch0_pred.jpg:** AynÄ± doÄŸrulama grubu iÃ§in modelin tahminlerini gÃ¶steren bir gÃ¶rÃ¼ntÃ¼.\n",
    "\n",
    "**weights:** Bu muhtemelen eÄŸitilmiÅŸ model aÄŸÄ±rlÄ±klarÄ±nÄ± iÃ§eren bir dizindir. Bunlar, eÄŸitimden sonra modelin Ã¶ÄŸrenilen parametrelerini saklayan dosyalardÄ±r.\n",
    "\n",
    "**Ã–zet:**  Bu dizin, performans Ã¶lÃ§Ã¼mleri (eÄŸriler, karÄ±ÅŸÄ±klÄ±k matrisleri), eÄŸitim sÃ¼recinin gÃ¶rselleÅŸtirmeleri ve TensorBoard tarafÄ±ndan oluÅŸturulan bazÄ± dosyalar dahil olmak Ã¼zere bir YOLO modelinin eÄŸitiminden elde edilen Ã§eÅŸitli Ã§Ä±ktÄ±larÄ± iÃ§erir. Dizindeki gÃ¶rÃ¼ntÃ¼ler modelin ne kadar iyi performans gÃ¶sterdiÄŸini gÃ¶rselleÅŸtirmeye yardÄ±mcÄ± olurken, args.yaml ve results.csv dosyalarÄ± ayrÄ±ntÄ±lÄ± yapÄ±landÄ±rma ve sonuÃ§ verileri saÄŸlar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "_J35i8Ofhjxa",
    "outputId": "2eab8ebe-e3fe-496f-bbd7-e261e0b43021"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600):**\n",
    "\n",
    "Bu satÄ±r, Jupyter Notebook'ta bir gÃ¶rÃ¼ntÃ¼ gÃ¶rÃ¼ntÃ¼lemek iÃ§in IPython.display'deki Image iÅŸlevini kullanÄ±r.\n",
    "\n",
    "**filename=f'{HOME}/runs/detect/train/confusion_matrix.png': **\n",
    "\n",
    "EÄŸitim sÄ±rasÄ±nda oluÅŸturulan karÄ±ÅŸÄ±klÄ±k matrisi gÃ¶rÃ¼ntÃ¼sÃ¼ne giden yolu belirtir.\n",
    "\n",
    "**width=600** : GÃ¶rÃ¼ntÃ¼lenen gÃ¶rÃ¼ntÃ¼nÃ¼n geniÅŸliÄŸini 600 piksel olarak ayarlar, bu da gÃ¶rÃ¼ntÃ¼nÃ¼n not defterinde gÃ¶rÃ¼ntÃ¼lenmek Ã¼zere uygun ÅŸekilde Ã¶lÃ§eklendirilmesini saÄŸlar.\n",
    "\n",
    "\n",
    "**Confusion Matrix Interpretation:**(KarÄ±ÅŸÄ±klÄ±k Matrisi Yorumu:)\n",
    "KarÄ±ÅŸÄ±klÄ±k matrisi, tipik olarak ikili sÄ±nÄ±flandÄ±rma problemlerinde bir modelin performansÄ±nÄ± Ã¶zetlemek iÃ§in kullanÄ±lan 2x2'lik bir tablodur.\n",
    "\n",
    "**Axes:**\n",
    "**horizontal axis (True)** \n",
    "Yatay eksen (GerÃ§ek) gerÃ§ek etiketleri (zemin gerÃ§eÄŸi) temsil eder.\n",
    "\n",
    "**vertical axis (Predicted)**\n",
    "Dikey eksen (Tahmin Edilen) model tarafÄ±ndan tahmin edilen etiketleri temsil eder.\n",
    "\n",
    "Etiketlerin \"insan\" ve \"arka plan\" olmasÄ±, modelin nesneleri insan ya da insan olmayan (arka plan) olarak sÄ±nÄ±flandÄ±rdÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.\n",
    "\n",
    "\n",
    "**Matrix Values:**\n",
    "\n",
    "**Top-left (22):** Sol Ã¼st (22): Modelin \"insan \"Ä± \"insan\" olarak doÄŸru tahmin ettiÄŸi doÄŸru pozitiflerin (TP) sayÄ±sÄ±nÄ± temsil eder.\n",
    "\n",
    "**Top-right (18):** Modelin \"insan \"Ä± \"arka plan\" olarak yanlÄ±ÅŸ tahmin ettiÄŸi yanlÄ±ÅŸ negatiflerin (FN) sayÄ±sÄ±nÄ± temsil eder.\n",
    "\n",
    "**Bottom-left (5):** Sol alt (5): Modelin \"arka plan \"Ä± \"insan\" olarak yanlÄ±ÅŸ tahmin ettiÄŸi yanlÄ±ÅŸ pozitiflerin (FP) sayÄ±sÄ±nÄ± temsil eder.\n",
    "\n",
    "**Bottom-right (empty):** Modelin \"arka plan \"Ä± \"arka plan\" olarak doÄŸru tahmin ettiÄŸi gerÃ§ek negatifleri (TN) temsil eder. Ancak, bu matriste hiÃ§bir gerÃ§ek negatif gÃ¶rÃ¼nmemektedir, bu da tÃ¼m gerÃ§ek arka planlarÄ±n arka plan olarak doÄŸru ÅŸekilde tahmin edildiÄŸini veya deÄŸerlendirmede bulunmadÄ±klarÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**Ã–zet:**\n",
    "\n",
    "**Accuracy Insight** :(DoÄŸruluk Ä°Ã§gÃ¶rÃ¼sÃ¼:)\n",
    "KarÄ±ÅŸÄ±klÄ±k matrisi, modelin 27 kiÅŸiden 22'sini doÄŸru tanÄ±mladÄ±ÄŸÄ±nÄ± gÃ¶stermektedir (22 TP + 5 FP), bu da makul bir performansa iÅŸaret etmekle birlikte iyileÅŸtirme iÃ§in bir miktar alan olduÄŸunu gÃ¶stermektedir.\n",
    "\n",
    "Model 18 \"insan\" Ã¶rneÄŸini \"arka plan\" olarak yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmÄ±ÅŸtÄ±r, bu da belirli durumlarda insanlarÄ± ayÄ±rt etmekte zorlanabileceÄŸini gÃ¶stermektedir.\n",
    "\n",
    "\n",
    "**Further Analysis** (Daha Fazla Analiz)\n",
    "\n",
    "Modeli geliÅŸtirmek istiyorsanÄ±z, epok sayÄ±sÄ±nÄ± artÄ±rmayÄ±, Ã¶ÄŸrenme oranÄ±nÄ± ayarlamayÄ± veya modelin insanlar ile arka plan Ã¶ÄŸelerini daha iyi ayÄ±rt etmesine yardÄ±mcÄ± olmak iÃ§in daha Ã§eÅŸitli eÄŸitim verileri saÄŸlamayÄ± dÃ¼ÅŸÃ¼nÃ¼n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "A-urTWUkhRmn",
    "outputId": "23652185-3b4a-4789-e6cc-b89c0361ddc6"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GÃ¶rÃ¼ntÃ¼, bir Jupyter Notebook'ta gÃ¶sterildiÄŸi gibi 25 epok boyunca Ã§izilen bir dizi eÄŸitim ve doÄŸrulama Ã¶lÃ§Ã¼mÃ¼nÃ¼ gÃ¶stermektedir. Bu gÃ¶rÃ¼ntÃ¼yÃ¼ oluÅŸturan kod Ã¶ncekilere benzer, ancak farklÄ± bir gÃ¶rselleÅŸtirme kÃ¼mesi gÃ¶sterir.\n",
    "\n",
    "**Plot Descriptions:**\n",
    "\n",
    "**1-train/box_loss:**\n",
    "\n",
    "**Box Loss (Training):** Bu, tahmin edilen sÄ±nÄ±rlayÄ±cÄ± kutularÄ±n temel gerÃ§ek kutularÄ±yla ne kadar iyi eÅŸleÅŸtiÄŸini Ã¶lÃ§er. Daha dÃ¼ÅŸÃ¼k bir kayÄ±p, sÄ±nÄ±rlayÄ±cÄ± kutularÄ± tahmin etmede daha iyi doÄŸruluk olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "**2-train/cls_loss:** \n",
    "\n",
    "**Classification Loss (Training):** (SÄ±nÄ±flandÄ±rma KaybÄ± (EÄŸitim)): Bu, modelin nesneleri doÄŸru kategorilere ne kadar iyi sÄ±nÄ±flandÄ±rdÄ±ÄŸÄ±nÄ± Ã¶lÃ§er. Daha dÃ¼ÅŸÃ¼k bir kayÄ±p, daha iyi sÄ±nÄ±flandÄ±rma doÄŸruluÄŸunu gÃ¶sterir.\n",
    "\n",
    "**3-train/dfl_loss:**\n",
    "\n",
    "**Distribution Focal Loss (Training):** Bu, tahmin edilen sÄ±nÄ±rlayÄ±cÄ± kutularÄ± zemin gerÃ§eÄŸiyle daha doÄŸru bir ÅŸekilde hizalamaya odaklanan nesne algÄ±lama modellerinde kullanÄ±lan Ã¶zel bir kayÄ±ptÄ±r.\n",
    "\n",
    "**4-metrics/precision(B):** \n",
    "\n",
    "**Precision Metric:**(Hassasiyet MetriÄŸi): Hassasiyet, tÃ¼m pozitif tahminler iÃ§inde doÄŸru pozitif tespitlerin yÃ¼zdesini Ã¶lÃ§er. Daha yÃ¼ksek hassasiyet, daha az yanlÄ±ÅŸ pozitif olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "**5-metrics/recall(B):** \n",
    "\n",
    "**Recall Metric:**  (Geri Ã‡aÄŸÄ±rma MetriÄŸi:) Geri Ã§aÄŸÄ±rma, veri kÃ¼mesindeki tÃ¼m gerÃ§ek pozitiflerin iÃ§indeki gerÃ§ek pozitiflerin yÃ¼zdesini Ã¶lÃ§er. Daha yÃ¼ksek geri Ã§aÄŸÄ±rma, daha az yanlÄ±ÅŸ negatif olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "**6-val/box_loss:** \n",
    "\n",
    "**Box Loss (Validation):** (Kutu KaybÄ± (DoÄŸrulama)): EÄŸitim kutusu kaybÄ±na benzer, ancak bu doÄŸrulama kÃ¼mesi Ã¼zerinde Ã¶lÃ§Ã¼lÃ¼r. Modelin gÃ¶rÃ¼lmeyen verilere iyi genelleme yapÄ±p yapmadÄ±ÄŸÄ±nÄ± belirlemeye yardÄ±mcÄ± olur.\n",
    "\n",
    "**7-val/cls_loss:** \n",
    "\n",
    "**Classification Loss (Validation):** SÄ±nÄ±flandÄ±rma KaybÄ± (DoÄŸrulama): Bu, modelin yeni verilere ne kadar iyi genelleme yaptÄ±ÄŸÄ±nÄ± gÃ¶steren doÄŸrulama setindeki sÄ±nÄ±flandÄ±rma doÄŸruluÄŸunu Ã¶lÃ§er.\n",
    "\n",
    "**8-val/dfl_loss:** \n",
    "\n",
    "**Distribution Focal Loss (Validation):** (DaÄŸÄ±tÄ±m Odak KaybÄ± (DoÄŸrulama)|) : Bu, modelin doÄŸrulama kÃ¼mesindeki sÄ±nÄ±rlayÄ±cÄ± kutularÄ± ne kadar iyi tahmin ettiÄŸini Ã¶lÃ§er.\n",
    "\n",
    "**9-metrics/mAP50(B):**  \n",
    "\n",
    "**Mean Average Precision at IoU=0.50 (B):**  Bu, nesne algÄ±lamada kesinlik ve geri Ã§aÄŸÄ±rmayÄ± birleÅŸtiren yaygÄ±n bir metriktir. IoU=0,50 deÄŸeri daha yumuÅŸaktÄ±r ve tahmin edilen kutu zemin gerÃ§eÄŸiyle %50 veya daha fazla Ã¶rtÃ¼ÅŸÃ¼yorsa bir algÄ±lamayÄ± doÄŸru kabul eder.  \n",
    "\n",
    "**10-metrics/mAP50-95(B)**  \n",
    "\n",
    "**Mean Average Precision at IoU=0.50-0.95 (B):** IoU=0,50-0,95'te Ortalama Ortalama Hassasiyet (B): Bu metrik, 0,50 ila 0,95 arasÄ±ndaki bir dizi IoU eÅŸiÄŸini dikkate alarak daha katÄ±dÄ±r. Modelin farklÄ± kutu Ã¶rtÃ¼ÅŸme doÄŸruluÄŸu seviyelerindeki performansÄ±nÄ±n daha kapsamlÄ± bir gÃ¶rÃ¼nÃ¼mÃ¼nÃ¼ verir.\n",
    "\n",
    "**Summary **\n",
    "\n",
    "**Training Losses** EÄŸitim KayÄ±plarÄ±: EÄŸitim iÃ§in kayÄ±p eÄŸrileri (box_loss, cls_loss, dfl_loss) genellikle azalmaktadÄ±r, bu da modelin eÄŸitim sÃ¼recinde Ã¶ÄŸrendiÄŸini ve tahminlerini geliÅŸtirdiÄŸini gÃ¶stermektedir.\n",
    "\n",
    "**Validation Losses:**(DoÄŸrulama KayÄ±plarÄ±): DoÄŸrulama kaybÄ± eÄŸrileri, modelin gÃ¶rÃ¼lmeyen verilere ne kadar iyi genelleme yaptÄ±ÄŸÄ±na dair fikir verir. Ä°deal olarak, bunlar eÄŸitim kayÄ±plarÄ±yla benzer bir dÃ¼ÅŸÃ¼ÅŸ eÄŸilimi izlemelidir, ancak herhangi bir sapma (Ã¶rneÄŸin, artan doÄŸrulama kaybÄ±) aÅŸÄ±rÄ± uyuma iÅŸaret edebilir.\n",
    "\n",
    "**Precision and Recall:** (Kesinlik ve Geri Ã‡aÄŸÄ±rma): Bu metrikler, eÄŸitim sÄ±rasÄ±nda tipik olan bazÄ± deÄŸiÅŸkenlikler gÃ¶sterir. Modelin nesneleri doÄŸru tespit etme becerisini geliÅŸtirdiÄŸini gÃ¶steren genel bir artÄ±ÅŸ eÄŸilimi arzu edilir.\n",
    "\n",
    "**mAP Metrics:** mAP Metrikleri: mAP metrikleri, modelin Ã§eÅŸitli kesiÅŸme noktasÄ± eÅŸiklerinde (IoU) gÃ¶sterdiÄŸi performansÄ±n bir Ã¶zetini sunar. Bunlar ideal olarak zaman iÃ§inde artmalÄ± ve daha iyi tespit doÄŸruluÄŸu gÃ¶stermelidir.\n",
    "\n",
    "SaÄŸlanan grafikler, modelin eÄŸitim ve doÄŸrulama sÄ±rasÄ±nda ne kadar iyi performans gÃ¶sterdiÄŸine dair kapsamlÄ± bir genel bakÄ±ÅŸ sunar. Azalan kayÄ±plar ve artan hassasiyet/hatÄ±rlama gibi genel eÄŸilimler, etkili bir ÅŸekilde Ã¶ÄŸrenen bir modelin iÅŸaretleridir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "HI4nADCCj3F5",
    "outputId": "418ba99c-dda3-4177-e6d3-636c1e208070"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600):** \n",
    "Bu satÄ±r, belirtilen dizindeki val_batch0_pred.jpg gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ 600 piksel geniÅŸliÄŸinde gÃ¶rÃ¼ntÃ¼ler.\n",
    "\n",
    "## **Image Analysis:**\n",
    "\n",
    "GÃ¶rÃ¼ntÃ¼, modelin doÄŸrulama grubundaki Ã§eÅŸitli karelerde (veya segmentlerde) insanlarÄ± tespit etmeye yÃ¶nelik tahminlerini gÃ¶stermektedir. Tahminler, tespit edilen her bir kiÅŸinin yanÄ±nda \"insan\" etiketi ve bir gÃ¼ven puanÄ± (0 ile 1 arasÄ±nda deÄŸiÅŸen) bulunan kÄ±rmÄ±zÄ± sÄ±nÄ±rlayÄ±cÄ± kutularla temsil edilmektedir.\n",
    "\n",
    "## **Bounding Boxes:** (SÄ±nÄ±rlayÄ±cÄ± Kutular:)\n",
    "Tespit edilen nesnelerin etrafÄ±ndaki kÄ±rmÄ±zÄ± dikdÃ¶rtgenler, modelin \"insan\" nesnelerini tanÄ±mladÄ±ÄŸÄ± alanlarÄ± gÃ¶stermektedir. Bu kutular model tarafÄ±ndan yapÄ±lan tahminlerdir.\n",
    "\n",
    "## **Confidence Scores:**(GÃ¼ven PuanlarÄ±:)\n",
    "\n",
    "Human (\"Ä°nsan\") etiketlerinin yanÄ±ndaki sayÄ±lar (Ã¶rn. 0,4, 0,6, 0,7) modelin tahminlerinin gÃ¼ven seviyelerini temsil etmektedir. Daha yÃ¼ksek bir puan, modelin tespit edilen nesnenin gerÃ§ekten bir insan olduÄŸundan daha emin olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "## **Interpretation:**(Yorum) \n",
    "\n",
    "**Model Performance:** \n",
    "\n",
    "Model, sÄ±nÄ±rlayÄ±cÄ± kutular ve ilgili gÃ¼ven puanlarÄ± ile gÃ¶sterildiÄŸi gibi, karelerde birden fazla insanÄ± baÅŸarÄ±yla tespit etmiÅŸtir. \n",
    "\n",
    "BazÄ± gÃ¼ven puanlarÄ± nispeten dÃ¼ÅŸÃ¼ktÃ¼r (Ã¶rn. 0,3, 0,4), bu da modelin bazÄ± tespitlerinden emin olmadÄ±ÄŸÄ±nÄ± gÃ¶sterebilir. Bunun nedeni oklÃ¼zyon, dÃ¼ÅŸÃ¼k gÃ¶rÃ¼ntÃ¼ kalitesi veya zorlu aÃ§Ä±lar gibi faktÃ¶rler olabilir.\n",
    "\n",
    "**Possible Improvements:**(Olasi Iyilestirmeler)\n",
    "\n",
    "**Increase Confidence Threshold:** GÃ¼ven EÅŸiÄŸini ArtÄ±rÄ±n:  YanlÄ±ÅŸ pozitifleri azaltmak iÃ§in belirli bir gÃ¼ven seviyesinin altÄ±ndaki tahminleri filtrelemeyi dÃ¼ÅŸÃ¼nebilirsiniz.\n",
    "\n",
    "**Further Training:**:  Daha Fazla EÄŸitim: BirÃ§ok tahmin dÃ¼ÅŸÃ¼k gÃ¼ven gÃ¶steriyorsa, daha Ã§eÅŸitli verilerle ek eÄŸitim, modelin doÄŸruluÄŸunu ve gÃ¼venini artÄ±rmaya yardÄ±mcÄ± olabilir.\n",
    "\n",
    "**Adjust Model Parameters** Model Parametrelerini AyarlayÄ±n: Ã–ÄŸrenme oranÄ± veya yÄ±ÄŸÄ±n boyutu gibi hiperparametrelerin ayarlanmasÄ± da tespit doÄŸruluÄŸunu artÄ±rabilir.\n",
    "\n",
    "**Ã–zet** : SaÄŸlanan gÃ¶rÃ¼ntÃ¼, modelin genel olarak doÄŸrulama gÃ¶rÃ¼ntÃ¼lerindeki insanlarÄ± tespit edebildiÄŸini, ancak farklÄ± gÃ¼ven derecelerine sahip olduÄŸunu gÃ¶stermektedir. Bu Ã§Ä±ktÄ±, modelin gÃ¶rÃ¼nmeyen veriler Ã¼zerindeki performansÄ±nÄ± deÄŸerlendirmek ve modelin daha fazla iyileÅŸtirmeye ihtiyaÃ§ duyabileceÄŸi alanlarÄ± belirlemek iÃ§in kullanÄ±ÅŸlÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ODk1VTlevxn"
   },
   "source": [
    "## Validate Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpyuwrNlXc1P",
    "outputId": "6920587d-fd6f-499c-c6b8-4d2974e48531"
   },
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml:**\n",
    "\n",
    "**!yolo**: YOLO komut satÄ±rÄ± arayÃ¼zÃ¼nÃ¼ Ã§aÄŸÄ±rÄ±r.\n",
    "\n",
    "**task=detect**: GÃ¶revin nesne algÄ±lama olduÄŸunu belirtir.\n",
    "\n",
    "**mode=val**: Modun doÄŸrulama olduÄŸunu, yani modelin bir doÄŸrulama veri kÃ¼mesi Ã¼zerinde deÄŸerlendirileceÄŸini belirtir.\n",
    "\n",
    "**model={HOME}/runs/detect/train/weights/best.pt:** EÄŸitim sÃ¼reci sÄ±rasÄ±nda kaydedilen eÄŸitilmiÅŸ modelin aÄŸÄ±rlÄ±klar dosyasÄ±nÄ±n (best.pt) yolunu belirtir. Bu model doÄŸrulama iÃ§in kullanÄ±lacaktÄ±r.\n",
    "\n",
    "**data={dataset.location}/data.yaml**  DoÄŸrulama veri kÃ¼mesi hakkÄ±nda bilgi iÃ§eren veri yapÄ±landÄ±rma dosyasÄ±nÄ±n (data.yaml) yolunu belirtir.\n",
    "\n",
    "**Output Analysis:**(Cikti Analizi)\n",
    "\n",
    "**Ultralytics YOLOv8.0.196:** KullanÄ±lan YOLOv8 sÃ¼rÃ¼mÃ¼.\n",
    "\n",
    "**Environment Details:** \n",
    "\n",
    "**Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB):** Bu satÄ±r Python ortamÄ±, PyTorch sÃ¼rÃ¼mÃ¼ ve doÄŸrulama iÃ§in kullanÄ±lan CUDA Ã¶zellikli GPU (Tesla T4) hakkÄ±nda ayrÄ±ntÄ±lar saÄŸlar.\n",
    "\n",
    "**Model Summary:(Model Ozeti)168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs:** \n",
    "\n",
    "168 katman, 11125971 parametreler, 0 gradyan, 28,4 GFLOPs: Bu, katman sayÄ±sÄ±, toplam parametreler ve GFLOP (Giga Kayan Nokta Ä°ÅŸlemleri) cinsinden hesaplama karmaÅŸÄ±klÄ±ÄŸÄ± dahil olmak Ã¼zere YOLOv8 modelinin mimarisini Ã¶zetlemektedir.\n",
    "\n",
    "\n",
    "**Validation Process:**(Dogrulama Sureci) :\n",
    "\n",
    "**Scanning /content/datasets/analiz-2/valid/labels.cache** Model, belirtilen yolda bulunan doÄŸrulama veri kÃ¼mesini tarÄ±yor.\n",
    "\n",
    "**Images: 3, Instances: 27:**  GÃ¶rÃ¼ntÃ¼ler: 3, Ã–rnekler: 27: DoÄŸrulama veri kÃ¼mesinin 3 gÃ¶rÃ¼ntÃ¼ ve algÄ±lanacak nesnelerin 27 Ã¶rneÄŸini iÃ§erdiÄŸini gÃ¶sterir.\n",
    "\n",
    "**Box(P R):**  DoÄŸrulama sonuÃ§larÄ± Hassasiyet (P), Geri Ã‡aÄŸÄ±rma (R), mAP50 ve mAP50-95 metriklerini iÃ§erir:\n",
    "\n",
    "       **Precision (P) ** Kesinlik (P): 0,63; bu da tespit edilen nesnelerin %63'Ã¼nÃ¼n gerÃ§ek pozitif olduÄŸunu gÃ¶stermektedir.\n",
    "       \n",
    "       **Recall (R): ** HatÄ±rlama (R): 0,63, yani model veri kÃ¼mesindeki gerÃ§ek nesnelerin %63'Ã¼nÃ¼ doÄŸru bir ÅŸekilde tanÄ±mlamÄ±ÅŸtÄ±r.\n",
    "       \n",
    "       **mAP50: ** 0.488, 0.50'lik Birlik Ãœzerinde KesiÅŸme (IoU) eÅŸiÄŸinde ortalama Ortalama Hassasiyeti temsil eder.\n",
    "       \n",
    "       **mAP50-95** 0.145, 0.50 ila 0.95 arasÄ±nda birden fazla IoU eÅŸiÄŸi arasÄ±nda hesaplanan mAP'dir. Bu metrik, modelin performansÄ±nÄ±n daha kapsamlÄ± bir deÄŸerlendirmesini verir.\n",
    "       \n",
    "       \n",
    "       **Speed:** \n",
    "       \n",
    "       **0.4ms preprocess, 29.5ms inference, 0.0ms loss, 201.6ms postprocess per image: ** GÃ¶rÃ¼ntÃ¼ baÅŸÄ±na 0,4 ms Ã¶n iÅŸlem, 29,5 ms Ã§Ä±karÄ±m, 0,0 ms kayÄ±p, 201,6 ms son iÅŸlem: Bunlar, Ã¶n iÅŸleme, Ã§Ä±karÄ±m ve son iÅŸleme dahil olmak Ã¼zere doÄŸrulama sÃ¼recinin her aÅŸamasÄ± iÃ§in zamanlamalardÄ±r.\n",
    "       \n",
    "       \n",
    "       **Results:** \n",
    "       \n",
    "       **Results saved to runs/detect/val** Results saved to runs/detect/val: Metrikler ve gÃ¶rselleÅŸtirmeler de dahil olmak Ã¼zere doÄŸrulama sonuÃ§larÄ± daha sonra incelenmek Ã¼zere belirtilen dizine kaydedilir.\n",
    "       \n",
    "       **Documentation:** YOLO'daki doÄŸrulama modlarÄ± hakkÄ±nda daha fazla bilgi edinmek iÃ§in bir baÄŸlantÄ± verilmiÅŸtir: https://docs.ultralytics.com/modes/val.\n",
    "       \n",
    "## Ã–zet:\n",
    "\n",
    "Bu dizÃ¼stÃ¼ bilgisayar hÃ¼cresi, kÃ¼Ã§Ã¼k bir doÄŸrulama veri kÃ¼mesi (27 Ã¶rnekli 3 gÃ¶rÃ¼ntÃ¼) kullanarak bir YOLOv8 modelini doÄŸrulamaktadÄ±r. Modelin performansÄ± hassasiyet, geri Ã§aÄŸÄ±rma ve mAP deÄŸerleri gibi temel metriklerle Ã¶zetlenmiÅŸtir.\n",
    "\n",
    "SonuÃ§lar, mAP deÄŸerlerinin gÃ¶sterdiÄŸi gibi orta dÃ¼zeyde performans ile dengeli bir hassasiyet ve geri Ã§aÄŸÄ±rma gÃ¶stermektedir. Bu metrikler, modelin gÃ¶rÃ¼lmeyen verilere ne kadar iyi genelleme yaptÄ±ÄŸÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±labilir.\n",
    "\n",
    "SÃ¼reÃ§ ayrÄ±ca, modelin gerÃ§ek zamanlÄ± uygulamalardaki verimliliÄŸini deÄŸerlendirirken Ã¶nemli olabilecek hÄ±z Ã¶lÃ§Ã¼mlerini de kaydeder.\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eASbcWkQBq"
   },
   "source": [
    "## Inference with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wjc1ctZykYuf",
    "outputId": "f880da3d-f821-44a9-cbc4-6d0e0f44d9a2"
   },
   "source": [
    "%cd {HOME}\n",
    "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OZET:** \n",
    "\n",
    "Bu dizÃ¼stÃ¼ bilgisayar hÃ¼cresi, bir test gÃ¶rÃ¼ntÃ¼sÃ¼ Ã¼zerinde Ã¶zel bir YOLOv8 modeli kullanarak Ã§Ä±karÄ±m yapmaktadÄ±r. Model gÃ¶rÃ¼ntÃ¼deki 14 insanÄ± baÅŸarÄ±yla tespit etti ve sÃ¼reÃ§ bazÄ± Ã¶n iÅŸleme, Ã§Ä±karÄ±m ve son iÅŸleme adÄ±mlarÄ±nÄ± iÃ§eriyordu. Tespit edilen nesneler ve ilgili sÄ±nÄ±rlayÄ±cÄ± kutular da dahil olmak Ã¼zere sonuÃ§lar daha fazla inceleme iÃ§in kaydedilmiÅŸtir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYIo95n-I0S"
   },
   "source": [
    "**NOTE:** Let's take a look at few results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "jbVjEtPAkz3j",
    "outputId": "089580a8-dd30-49f7-f18d-891478e0e362"
   },
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the base path where the folders are located\n",
    "base_path = '/content/runs/detect/'\n",
    "\n",
    "# List all directories that start with 'predict' in the base path\n",
    "subfolders = [os.path.join(base_path, d) for d in os.listdir(base_path)\n",
    "              if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict')]\n",
    "\n",
    "# Find the latest folder by modification time\n",
    "latest_folder = max(subfolders, key=os.path.getmtime)\n",
    "\n",
    "image_paths = glob.glob(f'{latest_folder}/*.jpg')[:3]\n",
    "\n",
    "# Display each image\n",
    "for image_path in image_paths:\n",
    "    display(Image(filename=image_path, width=600))\n",
    "    print(\"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SaÄŸlanan  gÃ¶rÃ¼ntÃ¼, belirli bir dizindeki gÃ¶rÃ¼ntÃ¼leri gÃ¶rÃ¼ntÃ¼lemek iÃ§in Python kodunun kullanÄ±ldÄ±ÄŸÄ± ve muhtemelen bir nesne algÄ±lama modelinin sonuÃ§larÄ±nÄ± gÃ¶steren bir Jupyter Notebook hÃ¼cresini gÃ¶stermektedir. Kodun altÄ±nda, tespit edilen nesnelerin (insanlar) sÄ±nÄ±rlayÄ±cÄ± kutular ve gÃ¼ven puanlarÄ± ile aÃ§Ä±klandÄ±ÄŸÄ± bir gÃ¶rÃ¼ntÃ¼ var.\n",
    "\n",
    "\n",
    "**1-import glob and from IPython.display import Image, display:** \n",
    "\n",
    "**glob** Belirli bir desenle eÅŸleÅŸen dosyalarÄ± aramanÄ±za olanak tanÄ±yan dosya deseni eÅŸleÅŸtirmesi iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "**Image, display** Bunlar, gÃ¶rÃ¼ntÃ¼leri doÄŸrudan Jupyter Notebook iÃ§inde gÃ¶rÃ¼ntÃ¼lemek iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "\n",
    "**2-Base Path Definition:** \n",
    "\n",
    "**base_path = '/content/runs/detect/'** Modelin Ã§Ä±ktÄ± klasÃ¶rlerinin bulunduÄŸu dizini tanÄ±mlar.\n",
    "\n",
    "**3-List Directories:** \n",
    "\n",
    "**subfolders = [os.path.join(base_path, d) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict')]:**\n",
    "Bu satÄ±r, temel yolda \"predict\" adÄ±yla baÅŸlayan alt klasÃ¶rlerin bir listesini oluÅŸturur.\n",
    "\n",
    "**4-Find the Latest Folder:** \n",
    "\n",
    "**latest_folder = max(subfolders, key=os.path.getmtime)** \"Tahmin\" klasÃ¶rleri listesinden en son deÄŸiÅŸtirilen klasÃ¶rÃ¼ bulur.\n",
    "\n",
    "**5-Image Paths:** \n",
    "\n",
    "**image_paths = glob.glob(f'{latest_folder}/*.jpg')[:3]:** En son klasÃ¶rdeki ilk Ã¼Ã§ .jpg gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n yollarÄ±nÄ± alÄ±r.\n",
    "\n",
    "**6-Display Images:**\n",
    "\n",
    "**for image_path in image_paths::**  GÃ¶rÃ¼ntÃ¼ yollarÄ± listesi Ã¼zerinde yineleme yapar.\n",
    "\n",
    "**display(Image(filename=image_path, width=600)):**  Not defterindeki her gÃ¶rÃ¼ntÃ¼yÃ¼ 600 piksel geniÅŸliÄŸe Ã¶lÃ§eklendirilmiÅŸ olarak gÃ¶rÃ¼ntÃ¼ler.\n",
    "\n",
    "**print(\"\\n\")**  Daha iyi okunabilirlik iÃ§in gÃ¶rÃ¼ntÃ¼lenen gÃ¶rÃ¼ntÃ¼lerin arasÄ±na yeni satÄ±r ekler.\n",
    "\n",
    "\n",
    "## Image Analysis:\n",
    "\n",
    "Kodun altÄ±nda gÃ¶rÃ¼ntÃ¼lenen gÃ¶rÃ¼ntÃ¼, bir sahnede birden fazla insanÄ±n tespit edildiÄŸi bir nesne algÄ±lama modelinden elde edilen Ã§Ä±ktÄ±yÄ± gÃ¶stermektedir.\n",
    "\n",
    "**Bounding Boxes** KÄ±rmÄ±zÄ± dikdÃ¶rtgenler modelin insanlarÄ± tespit ettiÄŸi alanlarÄ± vurgulamaktadÄ±r.\n",
    "\n",
    "**Confidence Scores**(GÃ¼ven PuanlarÄ±)\n",
    "\"Ä°nsan\" etiketlerinin yanÄ±ndaki sayÄ±lar (Ã¶rneÄŸin, 0,61, 0,36, 0,56) modelin her bir tespit iÃ§in gÃ¼ven dÃ¼zeyini temsil eder. Daha yÃ¼ksek bir puan, modelin tespit edilen nesnenin gerÃ§ekten bir insan olduÄŸundan daha emin olduÄŸu anlamÄ±na gelir.\n",
    "\n",
    "**Ã–zet: Bu kod parÃ§acÄ±ÄŸÄ±, en son nesne algÄ±lama sonuÃ§larÄ±nÄ± bulma ve gÃ¶rÃ¼ntÃ¼leme iÅŸlemini otomatikleÅŸtirir. GÃ¶rÃ¼ntÃ¼lenen gÃ¶rÃ¼ntÃ¼, modelin karedeki birden fazla insanÄ± deÄŸiÅŸen gÃ¼ven dÃ¼zeyleriyle baÅŸarÄ±yla tespit ettiÄŸini doÄŸrular.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0tsVilOCPyq"
   },
   "source": [
    "## Deploy model on Roboflow\n",
    "\n",
    "Once you have finished training your YOLOv8 model, youâ€™ll have a set of trained weights ready for use. These weights will be in the `/runs/detect/train/weights/best.pt` folder of your project. You can upload your model weights to Roboflow Deploy to use your trained weights on our infinitely scalable infrastructure.\n",
    "\n",
    "The `.deploy()` function in the [Roboflow pip package](https://docs.roboflow.com/python) now supports uploading YOLOv8 weights.\n",
    "\n",
    "To upload model weights, add the following code to the â€œInference with Custom Modelâ€ section in the aforementioned notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:42.840915Z",
     "start_time": "2024-08-13T15:53:41.658580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EhBAJ2gCPZh",
    "outputId": "0c064d40-7a1e-4697-fcab-afab40ba50b4"
   },
   "source": [
    "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5kOhjkmcV1l"
   },
   "source": [
    "#While your deployment is processing, checkout the deployment docs to take your model to most destinations https://docs.roboflow.com/inference"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4bpUIibcV1l"
   },
   "source": [
    "#Run inference on your model on a persistant, auto-scaling, cloud API\n",
    "\n",
    "#load model\n",
    "model = project.version(dataset.version).model\n",
    "\n",
    "#choose random test set image\n",
    "import os, random\n",
    "test_set_loc = dataset.location + \"/test/images/\"\n",
    "random_test_image = random.choice(os.listdir(test_set_loc))\n",
    "print(\"running inference on \" + random_test_image)\n",
    "\n",
    "pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n",
    "pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii6oacluX2Fn"
   },
   "source": [
    "# Deploy Your Model to the Edge\n",
    "\n",
    "In addition to using the Roboflow hosted API for deployment, you can use [Roboflow Inference](https://inference.roboflow.com), an open source inference solution that has powered millions of API calls in production environments. Inference works with CPU and GPU, giving you immediate access to a range of devices, from the NVIDIA Jetson to TRT-compatible devices to ARM CPU devices.\n",
    "\n",
    "With Roboflow Inference, you can self-host and deploy your model on-device. You can deploy applications using the [Inference Docker containers](https://inference.roboflow.com/quickstart/docker/) or the pip package.\n",
    "\n",
    "For example, to install Inference on a device with an NVIDIA GPU, we can use:\n",
    "\n",
    "```\n",
    "docker pull roboflow/roboflow-inference-server-gpu\n",
    "```\n",
    "\n",
    "Then we can run inference via HTTP:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "workspace_id = \"\"\n",
    "model_id = \"\"\n",
    "image_url = \"\"\n",
    "confidence = 0.75\n",
    "api_key = \"\"\n",
    "\n",
    "infer_payload = {\n",
    "    \"image\": {\n",
    "        \"type\": \"url\",\n",
    "        \"value\": image_url,\n",
    "    },\n",
    "    \"confidence\": confidence,\n",
    "    \"iou_threshold\": iou_thresh,\n",
    "    \"api_key\": api_key,\n",
    "}\n",
    "res = requests.post(\n",
    "    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n",
    "    json=infer_object_detection_payload,\n",
    ")\n",
    "\n",
    "predictions = res.json()\n",
    "```\n",
    "\n",
    "Above, set your Roboflow workspace ID, model ID, and API key.\n",
    "\n",
    "- [Find your workspace and model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids?ref=blog.roboflow.com)\n",
    "- [Find your API key](https://docs.roboflow.com/api-reference/authentication?ref=blog.roboflow.com#retrieve-an-api-key)\n",
    "\n",
    "Also, set the URL of an image on which you want to run inference. This can be a local file.\n",
    "\n",
    "_To use your YOLOv5 model commercially with Inference, you will need a Roboflow Enterprise license, through which you gain a pass-through license for using YOLOv5. An enterprise license also grants you access to features like advanced device management, multi-model containers, auto-batch inference, and more._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP1kp4mjX2Fn"
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQgOj_xSNDg"
   },
   "source": [
    "## ğŸ† Congratulations\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
    "\n",
    "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
    "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
    "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
    "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
    "\n",
    "### Convert data formats\n",
    "\n",
    "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
    "\n",
    "### Connect computer vision to your project logic\n",
    "\n",
    "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
