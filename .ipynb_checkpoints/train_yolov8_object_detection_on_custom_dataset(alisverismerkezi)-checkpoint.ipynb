{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe9vkEvFABbN"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# How to Train YOLOv8 Object Detection on a Custom Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n",
    "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
    "\n",
    "## ‚ö†Ô∏è Disclaimer\n",
    "\n",
    "YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
    "\n",
    "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
    "\n",
    "## Accompanying Blog Post\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Object Detection, concurrently.\n",
    "\n",
    "## Pro Tip: Use GPU Acceleration\n",
    "\n",
    "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
    "\n",
    "## Steps in this Tutorial\n",
    "\n",
    "In this tutorial, we are going to cover:\n",
    "\n",
    "- Before you start\n",
    "- Install YOLOv8\n",
    "- CLI Basics\n",
    "- Inference with Pre-trained COCO Model\n",
    "- Roboflow Universe\n",
    "- Preparing a custom dataset\n",
    "- Custom Training\n",
    "- Validate Custom Model\n",
    "- Inference with Custom Model\n",
    "\n",
    "**Let's begin!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyRdDYkqAKN4"
   },
   "source": [
    "## Before you start\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8cDtxLIBHgQ",
    "outputId": "95135e49-c7b1-4115-c4af-18f950485316"
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:52:49.012803Z",
     "start_time": "2024-08-13T15:52:49.007005Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "da87c813-6c0a-4902-c07d-29407ccb5ec4"
   },
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C3EO_2zNChu"
   },
   "source": [
    "## Install YOLOv8\n",
    "\n",
    "‚ö†Ô∏è YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **03.01.2024** with version **YOLOv8.0.196**.\n",
    "\n",
    "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
    "\n",
    "YOLOv8 can be installed in two ways‚Ää-‚Ääfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:52:56.756558Z",
     "start_time": "2024-08-13T15:52:53.635485Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdSMcABDNKW-",
    "outputId": "002d1741-8bb4-4be5-f489-1e17dc7ea9b1"
   },
   "source": [
    "# Pip install method (recommended)\n",
    "\n",
    "!pip install ultralytics==8.0.196\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVvaIYEEPOty"
   },
   "source": [
    "# Git clone method (for development)\n",
    "\n",
    "# %cd {HOME}\n",
    "# !git clone github.com/ultralytics/ultralytics\n",
    "# %cd {HOME}/ultralytics\n",
    "# !pip install -e .\n",
    "\n",
    "# from IPython import display\n",
    "# display.clear_output()\n",
    "\n",
    "# import ultralytics\n",
    "# ultralytics.checks()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:00.450582Z",
     "start_time": "2024-08-13T15:53:00.447113Z"
    },
    "id": "VOEYrlBoP9-E"
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnnZSm5OQfPQ"
   },
   "source": [
    "## CLI Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K33S7zlkQku0"
   },
   "source": [
    "If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started. Read more about CLI in [Ultralytics YOLO Docs](https://docs.ultralytics.com/usage/cli/).\n",
    "\n",
    "```\n",
    "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
    "          classify       predict        yolov8n-cls.yaml  args...\n",
    "          segment        val            yolov8n-seg.yaml  args...\n",
    "                         export         yolov8n.pt        format=onnx  args...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5RGYA6sPgEd"
   },
   "source": [
    "## Inference with Pre-trained COCO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT1qD4toTTw0"
   },
   "source": [
    "### üíª CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaE1kLS8R4CV"
   },
   "source": [
    "`yolo mode=predict` runs YOLOv8 inference on a variety of sources, downloading models automatically from the latest YOLOv8 release, and saving results to `runs/predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDbMt_M6PiXb",
    "outputId": "3c2fd973-0d92-4f5c-dbc4-8800c7de87f6"
   },
   "source": [
    "%cd {HOME}\n",
    "!yolo task=detect mode=predict model=yolov8n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "LyopYpK1TQrB",
    "outputId": "287966c3-84c5-4bb4-8163-8911acb4d37a"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename='runs/detect/predict/dog.jpeg', height=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFMBYQtMVL-B"
   },
   "source": [
    "### üêç Python SDK\n",
    "\n",
    "The simplest way of simply using YOLOv8 directly in a Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx9NWF-sVN6Y",
    "outputId": "f8b8e341-ae74-4b33-92ea-0e0c10a63902"
   },
   "source": [
    "model = YOLO(f'{HOME}/yolov8n.pt')\n",
    "results = model.predict(source='https://media.roboflow.com/notebooks/examples/dog.jpeg', conf=0.25)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAi4PvrItTCf",
    "outputId": "3a1a1c21-be10-437f-aa14-4995d5321789"
   },
   "source": [
    "results[0].boxes.xyxy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqT2M01K1LUb",
    "outputId": "ac8d0988-8be7-4fec-c62b-2cd8fe9b5371"
   },
   "source": [
    "results[0].boxes.conf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKIwJ5yw1PMb",
    "outputId": "ee27ea55-240f-43fd-d9a3-e8b8a73149fb"
   },
   "source": [
    "results[0].boxes.cls"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2Xtaekw3271"
   },
   "source": [
    "## Roboflow Universe\n",
    "\n",
    "Need data for your project? Before spending time on annotating, check out Roboflow Universe, a repository of more than 110,000 open-source datasets that you can use in your projects. You'll find datasets containing everything from annotated cracks in concrete to plant images with disease annotations.\n",
    "\n",
    "\n",
    "[![Roboflow Universe](https://media.roboflow.com/notebooks/template/uni-banner-frame.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672878480290)](https://universe.roboflow.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JHICVjZbVKn"
   },
   "source": [
    "## Preparing a custom¬†dataset\n",
    "\n",
    "Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process as straightforward and fast as possible. Let me show you how!\n",
    "\n",
    "### Step 1: Creating project\n",
    "\n",
    "Before you start, you need to create a Roboflow [account](https://app.roboflow.com/login). Once you do that, you can create a new project in the Roboflow [dashboard](https://app.roboflow.com/). Keep in mind to choose the right project type. In our case, Object Detection.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/creating-project.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929799852\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 2: Uploading images\n",
    "\n",
    "Next, add the data to your newly created project. You can do it via API or through our [web interface](https://docs.roboflow.com/adding-data/object-detection).\n",
    "\n",
    "If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/uploading-images.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672929808290\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 3: Labeling\n",
    "\n",
    "If you only have images, you can label them in [Roboflow Annotate](https://docs.roboflow.com/annotate).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://user-images.githubusercontent.com/26109316/210901980-04861efd-dfc0-4a01-9373-13a36b5e1df4.gif\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 4: Generate new dataset version\n",
    "\n",
    "Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is completely optional, however, it can allow you to significantly improve the robustness of your model.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/generate-new-version.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1673003597834\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "### Step 5: Exporting dataset\n",
    "\n",
    "Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training. Click `Export` and select the `YOLO v5 PyTorch` dataset format.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img\n",
    "    width=\"640\"\n",
    "    src=\"https://media.roboflow.com/preparing-custom-dataset-example/export.gif?ik-sdk-version=javascript-1.4.3&updatedAt=1672943313709\"\n",
    "  >\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:16.466270Z",
     "start_time": "2024-08-13T15:53:12.397419Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSd93ZJzZZKt",
    "outputId": "f5f44c10-3908-4d95-d5db-5862bd825387"
   },
   "source": [
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets\n",
    "\n",
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"0i8v0fEn4mty6Zt83ACV\")\n",
    "project = rf.workspace(\"myworkspace-bkrfh\").project(\"analiz-ssev8\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUjFBKKqXa-u"
   },
   "source": [
    "## Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2YkphuiaE7_",
    "outputId": "f989e4d8-f9e5-47ea-96e2-0062e0c0aff5"
   },
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=25 imgsz=800 plots=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kod par√ßacƒ±ƒüƒ±, nesne algƒ±lama g√∂revi i√ßin √∂zel bir veri k√ºmesi √ºzerinde bir YOLOv8 modelini eƒüitmek i√ßin kullanƒ±lƒ±r. Eƒüitim, 800x800 piksele yeniden boyutlandƒ±rƒ±lmƒ±≈ü g√∂r√ºnt√ºler √ºzerinde 25 epok boyunca √ßalƒ±≈üacaktƒ±r. Komut, modelin nasƒ±l √∂ƒürendiƒüini g√∂rselle≈ütirmeye yardƒ±mcƒ± olmak i√ßin eƒüitim sƒ±rasƒ±nda otomatik olarak grafikler olu≈üturacak ≈üekilde ayarlanmƒ±≈ütƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MScstfHhArr",
    "outputId": "05bd1c5d-a582-45f8-cd1b-7accf6a34809"
   },
   "source": [
    "!ls {HOME}/runs/detect/train/"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**args.yaml**: Genellikle eƒüitim s√ºrecinde kullanƒ±lan model yapƒ±landƒ±rmasƒ±, √∂ƒürenme oranƒ±, yƒ±ƒüƒ±n boyutu vb. gibi arg√ºmanlarƒ± veya hiperparametreleri depolayan bir YAML dosyasƒ±.\n",
    "\n",
    "**confusion_matrix_normalized.png ve confusion_matrix.png**: Bunlar eƒüitim sƒ±rasƒ±nda olu≈üturulan karƒ±≈üƒ±klƒ±k matrislerini temsil eden g√∂r√ºnt√ºlerdir (PNG formatƒ±nda).\n",
    "\n",
    "**Confusion Matrix:(Karƒ±≈üƒ±klƒ±k Matrisi)**: Ger√ßek ve tahmin edilen sƒ±nƒ±flandƒ±rmalarƒ± g√∂stererek bir sƒ±nƒ±flandƒ±rma algoritmasƒ±nƒ±n performansƒ±nƒ± deƒüerlendirmek i√ßin kullanƒ±lan bir tablo.\n",
    "\n",
    "**Normalized:** Normalle≈ütirilmi≈ü karƒ±≈üƒ±klƒ±k matrisi, ham sayƒ±lar yerine tahminlerin oranlarƒ±nƒ± g√∂sterir.\n",
    "\n",
    "**events.out.tfevents:** Bu, eƒüitim sƒ±rasƒ±nda √∂l√ß√ºmleri g√∂rselle≈ütirmek i√ßin bir ara√ß olan TensorBoard tarafƒ±ndan olu≈üturulan bir dosyadƒ±r. Dosya skaler deƒüerler, histogramlar, g√∂r√ºnt√ºler ve daha fazlasƒ± gibi olay verilerini i√ßerir.\n",
    "\n",
    "\n",
    "**F1_curve.png:** Eƒüitim sƒ±rasƒ±nda F1 skoru eƒürisini temsil eden bir g√∂r√ºnt√º.  \n",
    "\n",
    "**F1 Score(F1 Puanƒ±)**: Kesinlik ve geri √ßaƒüƒ±rmayƒ± dengeleyen, √∂zellikle dengesiz veri k√ºmelerinde yararlƒ± olan bir metrik.\n",
    "\n",
    "**labels_correlogram.jpg and labels.jpg:** \n",
    "\n",
    "**Labels Correlagram:**(Etiketler Korelagramƒ±): Farklƒ± etiketler veya sƒ±nƒ±flar arasƒ±ndaki korelasyonu g√∂steren g√∂rsel bir temsil.\n",
    "\n",
    "**Labels.jpg:** B√ºy√ºk olasƒ±lƒ±kla veri k√ºmesinde bulunan etiketlerin g√∂rselle≈ütirilmesi.\n",
    "\n",
    "**P_curve.png, PR_curve.png, R_curve.png:**\n",
    "\n",
    "**P_curve.png** Eƒüitim sƒ±rasƒ±nda hassasiyet eƒürisi.\n",
    "\n",
    "**PR_curve.png:** Precision-Recall curve(Hassasiyet-Geri √áaƒüƒ±rma eƒürisi), farklƒ± e≈üikler i√ßin hassasiyet ve geri √ßaƒüƒ±rma arasƒ±ndaki deƒüi≈ü toku≈üu g√∂steren grafiksel bir g√∂sterimdir.\n",
    "\n",
    "**R_curve.png:** Eƒüitim sƒ±rasƒ±nda hatƒ±rlama eƒürisi.\n",
    "\n",
    "**results.csv and results.png:** \n",
    "\n",
    "**results.csv:** Doƒüruluk, kesinlik, geri √ßaƒüƒ±rma gibi metrikler de dahil olmak √ºzere eƒüitim s√ºrecinin ayrƒ±ntƒ±lƒ± sonu√ßlarƒ±nƒ± i√ßeren bir CSV dosyasƒ±.\n",
    "\n",
    "**results.png:** Eƒüitim sonu√ßlarƒ±nƒ±n g√∂rsel bir temsili (muhtemelen bir √∂zet).\n",
    "\n",
    "**train_batch0.jpg, train_batch1.jpg, train_batch2.jpg, etc.:**\n",
    "\n",
    "Bunlar eƒüitim gruplarƒ±ndan g√∂r√ºnt√ºlerdir. Muhtemelen modelin eƒüitim sƒ±rasƒ±nda bu g√∂r√ºnt√ºlerdeki nesneleri nasƒ±l tahmin ettiƒüine dair √∂rnekler g√∂sterirler.\n",
    "\n",
    "**val_batch0_labels.jpg and val_batch0_pred.jpg:**\n",
    "\n",
    "**val_batch0_labels.jpg:** Bir doƒürulama grubu i√ßin temel ger√ßek etiketlerini g√∂steren bir g√∂r√ºnt√º.\n",
    "\n",
    "**val_batch0_pred.jpg:** Aynƒ± doƒürulama grubu i√ßin modelin tahminlerini g√∂steren bir g√∂r√ºnt√º.\n",
    "\n",
    "**weights:** Bu muhtemelen eƒüitilmi≈ü model aƒüƒ±rlƒ±klarƒ±nƒ± i√ßeren bir dizindir. Bunlar, eƒüitimden sonra modelin √∂ƒürenilen parametrelerini saklayan dosyalardƒ±r.\n",
    "\n",
    "**√ñzet:**  Bu dizin, performans √∂l√ß√ºmleri (eƒüriler, karƒ±≈üƒ±klƒ±k matrisleri), eƒüitim s√ºrecinin g√∂rselle≈ütirmeleri ve TensorBoard tarafƒ±ndan olu≈üturulan bazƒ± dosyalar dahil olmak √ºzere bir YOLO modelinin eƒüitiminden elde edilen √ße≈üitli √ßƒ±ktƒ±larƒ± i√ßerir. Dizindeki g√∂r√ºnt√ºler modelin ne kadar iyi performans g√∂sterdiƒüini g√∂rselle≈ütirmeye yardƒ±mcƒ± olurken, args.yaml ve results.csv dosyalarƒ± ayrƒ±ntƒ±lƒ± yapƒ±landƒ±rma ve sonu√ß verileri saƒülar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "_J35i8Ofhjxa",
    "outputId": "2eab8ebe-e3fe-496f-bbd7-e261e0b43021"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600):**\n",
    "\n",
    "Bu satƒ±r, Jupyter Notebook'ta bir g√∂r√ºnt√º g√∂r√ºnt√ºlemek i√ßin IPython.display'deki Image i≈ülevini kullanƒ±r.\n",
    "\n",
    "**filename=f'{HOME}/runs/detect/train/confusion_matrix.png': **\n",
    "\n",
    "Eƒüitim sƒ±rasƒ±nda olu≈üturulan karƒ±≈üƒ±klƒ±k matrisi g√∂r√ºnt√ºs√ºne giden yolu belirtir.\n",
    "\n",
    "**width=600** : G√∂r√ºnt√ºlenen g√∂r√ºnt√ºn√ºn geni≈üliƒüini 600 piksel olarak ayarlar, bu da g√∂r√ºnt√ºn√ºn not defterinde g√∂r√ºnt√ºlenmek √ºzere uygun ≈üekilde √∂l√ßeklendirilmesini saƒülar.\n",
    "\n",
    "\n",
    "**Confusion Matrix Interpretation:**(Karƒ±≈üƒ±klƒ±k Matrisi Yorumu:)\n",
    "Karƒ±≈üƒ±klƒ±k matrisi, tipik olarak ikili sƒ±nƒ±flandƒ±rma problemlerinde bir modelin performansƒ±nƒ± √∂zetlemek i√ßin kullanƒ±lan 2x2'lik bir tablodur.\n",
    "\n",
    "**Axes:**\n",
    "**horizontal axis (True)** \n",
    "Yatay eksen (Ger√ßek) ger√ßek etiketleri (zemin ger√ßeƒüi) temsil eder.\n",
    "\n",
    "**vertical axis (Predicted)**\n",
    "Dikey eksen (Tahmin Edilen) model tarafƒ±ndan tahmin edilen etiketleri temsil eder.\n",
    "\n",
    "Etiketlerin \"insan\" ve \"arka plan\" olmasƒ±, modelin nesneleri insan ya da insan olmayan (arka plan) olarak sƒ±nƒ±flandƒ±rdƒ±ƒüƒ±nƒ± g√∂stermektedir.\n",
    "\n",
    "\n",
    "**Matrix Values:**\n",
    "\n",
    "**Top-left (22):** Sol √ºst (22): Modelin \"insan \"ƒ± \"insan\" olarak doƒüru tahmin ettiƒüi doƒüru pozitiflerin (TP) sayƒ±sƒ±nƒ± temsil eder.\n",
    "\n",
    "**Top-right (18):** Modelin \"insan \"ƒ± \"arka plan\" olarak yanlƒ±≈ü tahmin ettiƒüi yanlƒ±≈ü negatiflerin (FN) sayƒ±sƒ±nƒ± temsil eder.\n",
    "\n",
    "**Bottom-left (5):** Sol alt (5): Modelin \"arka plan \"ƒ± \"insan\" olarak yanlƒ±≈ü tahmin ettiƒüi yanlƒ±≈ü pozitiflerin (FP) sayƒ±sƒ±nƒ± temsil eder.\n",
    "\n",
    "**Bottom-right (empty):** Modelin \"arka plan \"ƒ± \"arka plan\" olarak doƒüru tahmin ettiƒüi ger√ßek negatifleri (TN) temsil eder. Ancak, bu matriste hi√ßbir ger√ßek negatif g√∂r√ºnmemektedir, bu da t√ºm ger√ßek arka planlarƒ±n arka plan olarak doƒüru ≈üekilde tahmin edildiƒüini veya deƒüerlendirmede bulunmadƒ±klarƒ±nƒ± g√∂sterir.\n",
    "\n",
    "**√ñzet:**\n",
    "\n",
    "**Accuracy Insight** :(Doƒüruluk ƒ∞√ßg√∂r√ºs√º:)\n",
    "Karƒ±≈üƒ±klƒ±k matrisi, modelin 27 ki≈üiden 22'sini doƒüru tanƒ±mladƒ±ƒüƒ±nƒ± g√∂stermektedir (22 TP + 5 FP), bu da makul bir performansa i≈üaret etmekle birlikte iyile≈ütirme i√ßin bir miktar alan olduƒüunu g√∂stermektedir.\n",
    "\n",
    "Model 18 \"insan\" √∂rneƒüini \"arka plan\" olarak yanlƒ±≈ü sƒ±nƒ±flandƒ±rmƒ±≈ütƒ±r, bu da belirli durumlarda insanlarƒ± ayƒ±rt etmekte zorlanabileceƒüini g√∂stermektedir.\n",
    "\n",
    "\n",
    "**Further Analysis** (Daha Fazla Analiz)\n",
    "\n",
    "Modeli geli≈ütirmek istiyorsanƒ±z, epok sayƒ±sƒ±nƒ± artƒ±rmayƒ±, √∂ƒürenme oranƒ±nƒ± ayarlamayƒ± veya modelin insanlar ile arka plan √∂ƒüelerini daha iyi ayƒ±rt etmesine yardƒ±mcƒ± olmak i√ßin daha √ße≈üitli eƒüitim verileri saƒülamayƒ± d√º≈ü√ºn√ºn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "A-urTWUkhRmn",
    "outputId": "23652185-3b4a-4789-e6cc-b89c0361ddc6"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G√∂r√ºnt√º, bir Jupyter Notebook'ta g√∂sterildiƒüi gibi 25 epok boyunca √ßizilen bir dizi eƒüitim ve doƒürulama √∂l√ß√ºm√ºn√º g√∂stermektedir. Bu g√∂r√ºnt√ºy√º olu≈üturan kod √∂ncekilere benzer, ancak farklƒ± bir g√∂rselle≈ütirme k√ºmesi g√∂sterir.\n",
    "\n",
    "**Plot Descriptions:**\n",
    "\n",
    "**1-train/box_loss:**\n",
    "\n",
    "**Box Loss (Training):** Bu, tahmin edilen sƒ±nƒ±rlayƒ±cƒ± kutularƒ±n temel ger√ßek kutularƒ±yla ne kadar iyi e≈üle≈ütiƒüini √∂l√ßer. Daha d√º≈ü√ºk bir kayƒ±p, sƒ±nƒ±rlayƒ±cƒ± kutularƒ± tahmin etmede daha iyi doƒüruluk olduƒüunu g√∂sterir.\n",
    "\n",
    "**2-train/cls_loss:** \n",
    "\n",
    "**Classification Loss (Training):** (Sƒ±nƒ±flandƒ±rma Kaybƒ± (Eƒüitim)): Bu, modelin nesneleri doƒüru kategorilere ne kadar iyi sƒ±nƒ±flandƒ±rdƒ±ƒüƒ±nƒ± √∂l√ßer. Daha d√º≈ü√ºk bir kayƒ±p, daha iyi sƒ±nƒ±flandƒ±rma doƒüruluƒüunu g√∂sterir.\n",
    "\n",
    "**3-train/dfl_loss:**\n",
    "\n",
    "**Distribution Focal Loss (Training):** Bu, tahmin edilen sƒ±nƒ±rlayƒ±cƒ± kutularƒ± zemin ger√ßeƒüiyle daha doƒüru bir ≈üekilde hizalamaya odaklanan nesne algƒ±lama modellerinde kullanƒ±lan √∂zel bir kayƒ±ptƒ±r.\n",
    "\n",
    "**4-metrics/precision(B):** \n",
    "\n",
    "**Precision Metric:**(Hassasiyet Metriƒüi): Hassasiyet, t√ºm pozitif tahminler i√ßinde doƒüru pozitif tespitlerin y√ºzdesini √∂l√ßer. Daha y√ºksek hassasiyet, daha az yanlƒ±≈ü pozitif olduƒüunu g√∂sterir.\n",
    "\n",
    "**5-metrics/recall(B):** \n",
    "\n",
    "**Recall Metric:**  (Geri √áaƒüƒ±rma Metriƒüi:) Geri √ßaƒüƒ±rma, veri k√ºmesindeki t√ºm ger√ßek pozitiflerin i√ßindeki ger√ßek pozitiflerin y√ºzdesini √∂l√ßer. Daha y√ºksek geri √ßaƒüƒ±rma, daha az yanlƒ±≈ü negatif olduƒüunu g√∂sterir.\n",
    "\n",
    "**6-val/box_loss:** \n",
    "\n",
    "**Box Loss (Validation):** (Kutu Kaybƒ± (Doƒürulama)): Eƒüitim kutusu kaybƒ±na benzer, ancak bu doƒürulama k√ºmesi √ºzerinde √∂l√ß√ºl√ºr. Modelin g√∂r√ºlmeyen verilere iyi genelleme yapƒ±p yapmadƒ±ƒüƒ±nƒ± belirlemeye yardƒ±mcƒ± olur.\n",
    "\n",
    "**7-val/cls_loss:** \n",
    "\n",
    "**Classification Loss (Validation):** Sƒ±nƒ±flandƒ±rma Kaybƒ± (Doƒürulama): Bu, modelin yeni verilere ne kadar iyi genelleme yaptƒ±ƒüƒ±nƒ± g√∂steren doƒürulama setindeki sƒ±nƒ±flandƒ±rma doƒüruluƒüunu √∂l√ßer.\n",
    "\n",
    "**8-val/dfl_loss:** \n",
    "\n",
    "**Distribution Focal Loss (Validation):** (Daƒüƒ±tƒ±m Odak Kaybƒ± (Doƒürulama)|) : Bu, modelin doƒürulama k√ºmesindeki sƒ±nƒ±rlayƒ±cƒ± kutularƒ± ne kadar iyi tahmin ettiƒüini √∂l√ßer.\n",
    "\n",
    "**9-metrics/mAP50(B):**  \n",
    "\n",
    "**Mean Average Precision at IoU=0.50 (B):**  Bu, nesne algƒ±lamada kesinlik ve geri √ßaƒüƒ±rmayƒ± birle≈ütiren yaygƒ±n bir metriktir. IoU=0,50 deƒüeri daha yumu≈üaktƒ±r ve tahmin edilen kutu zemin ger√ßeƒüiyle %50 veya daha fazla √∂rt√º≈ü√ºyorsa bir algƒ±lamayƒ± doƒüru kabul eder.  \n",
    "\n",
    "**10-metrics/mAP50-95(B)**  \n",
    "\n",
    "**Mean Average Precision at IoU=0.50-0.95 (B):** IoU=0,50-0,95'te Ortalama Ortalama Hassasiyet (B): Bu metrik, 0,50 ila 0,95 arasƒ±ndaki bir dizi IoU e≈üiƒüini dikkate alarak daha katƒ±dƒ±r. Modelin farklƒ± kutu √∂rt√º≈üme doƒüruluƒüu seviyelerindeki performansƒ±nƒ±n daha kapsamlƒ± bir g√∂r√ºn√ºm√ºn√º verir.\n",
    "\n",
    "**Summary **\n",
    "\n",
    "**Training Losses** Eƒüitim Kayƒ±plarƒ±: Eƒüitim i√ßin kayƒ±p eƒürileri (box_loss, cls_loss, dfl_loss) genellikle azalmaktadƒ±r, bu da modelin eƒüitim s√ºrecinde √∂ƒürendiƒüini ve tahminlerini geli≈ütirdiƒüini g√∂stermektedir.\n",
    "\n",
    "**Validation Losses:**(Doƒürulama Kayƒ±plarƒ±): Doƒürulama kaybƒ± eƒürileri, modelin g√∂r√ºlmeyen verilere ne kadar iyi genelleme yaptƒ±ƒüƒ±na dair fikir verir. ƒ∞deal olarak, bunlar eƒüitim kayƒ±plarƒ±yla benzer bir d√º≈ü√º≈ü eƒüilimi izlemelidir, ancak herhangi bir sapma (√∂rneƒüin, artan doƒürulama kaybƒ±) a≈üƒ±rƒ± uyuma i≈üaret edebilir.\n",
    "\n",
    "**Precision and Recall:** (Kesinlik ve Geri √áaƒüƒ±rma): Bu metrikler, eƒüitim sƒ±rasƒ±nda tipik olan bazƒ± deƒüi≈ükenlikler g√∂sterir. Modelin nesneleri doƒüru tespit etme becerisini geli≈ütirdiƒüini g√∂steren genel bir artƒ±≈ü eƒüilimi arzu edilir.\n",
    "\n",
    "**mAP Metrics:** mAP Metrikleri: mAP metrikleri, modelin √ße≈üitli kesi≈üme noktasƒ± e≈üiklerinde (IoU) g√∂sterdiƒüi performansƒ±n bir √∂zetini sunar. Bunlar ideal olarak zaman i√ßinde artmalƒ± ve daha iyi tespit doƒüruluƒüu g√∂stermelidir.\n",
    "\n",
    "Saƒülanan grafikler, modelin eƒüitim ve doƒürulama sƒ±rasƒ±nda ne kadar iyi performans g√∂sterdiƒüine dair kapsamlƒ± bir genel bakƒ±≈ü sunar. Azalan kayƒ±plar ve artan hassasiyet/hatƒ±rlama gibi genel eƒüilimler, etkili bir ≈üekilde √∂ƒürenen bir modelin i≈üaretleridir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "HI4nADCCj3F5",
    "outputId": "418ba99c-dda3-4177-e6d3-636c1e208070"
   },
   "source": [
    "%cd {HOME}\n",
    "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600):** \n",
    "Bu satƒ±r, belirtilen dizindeki val_batch0_pred.jpg g√∂r√ºnt√ºs√ºn√º 600 piksel geni≈üliƒüinde g√∂r√ºnt√ºler.\n",
    "\n",
    "## **Image Analysis:**\n",
    "\n",
    "G√∂r√ºnt√º, modelin doƒürulama grubundaki √ße≈üitli karelerde (veya segmentlerde) insanlarƒ± tespit etmeye y√∂nelik tahminlerini g√∂stermektedir. Tahminler, tespit edilen her bir ki≈üinin yanƒ±nda \"insan\" etiketi ve bir g√ºven puanƒ± (0 ile 1 arasƒ±nda deƒüi≈üen) bulunan kƒ±rmƒ±zƒ± sƒ±nƒ±rlayƒ±cƒ± kutularla temsil edilmektedir.\n",
    "\n",
    "## **Bounding Boxes:** (Sƒ±nƒ±rlayƒ±cƒ± Kutular:)\n",
    "Tespit edilen nesnelerin etrafƒ±ndaki kƒ±rmƒ±zƒ± dikd√∂rtgenler, modelin \"insan\" nesnelerini tanƒ±mladƒ±ƒüƒ± alanlarƒ± g√∂stermektedir. Bu kutular model tarafƒ±ndan yapƒ±lan tahminlerdir.\n",
    "\n",
    "## **Confidence Scores:**(G√ºven Puanlarƒ±:)\n",
    "\n",
    "Human (\"ƒ∞nsan\") etiketlerinin yanƒ±ndaki sayƒ±lar (√∂rn. 0,4, 0,6, 0,7) modelin tahminlerinin g√ºven seviyelerini temsil etmektedir. Daha y√ºksek bir puan, modelin tespit edilen nesnenin ger√ßekten bir insan olduƒüundan daha emin olduƒüunu g√∂sterir.\n",
    "\n",
    "## **Interpretation:**(Yorum) \n",
    "\n",
    "**Model Performance:** \n",
    "\n",
    "Model, sƒ±nƒ±rlayƒ±cƒ± kutular ve ilgili g√ºven puanlarƒ± ile g√∂sterildiƒüi gibi, karelerde birden fazla insanƒ± ba≈üarƒ±yla tespit etmi≈ütir. \n",
    "\n",
    "Bazƒ± g√ºven puanlarƒ± nispeten d√º≈ü√ºkt√ºr (√∂rn. 0,3, 0,4), bu da modelin bazƒ± tespitlerinden emin olmadƒ±ƒüƒ±nƒ± g√∂sterebilir. Bunun nedeni okl√ºzyon, d√º≈ü√ºk g√∂r√ºnt√º kalitesi veya zorlu a√ßƒ±lar gibi fakt√∂rler olabilir.\n",
    "\n",
    "**Possible Improvements:**(Olasi Iyilestirmeler)\n",
    "\n",
    "**Increase Confidence Threshold:** G√ºven E≈üiƒüini Artƒ±rƒ±n:  Yanlƒ±≈ü pozitifleri azaltmak i√ßin belirli bir g√ºven seviyesinin altƒ±ndaki tahminleri filtrelemeyi d√º≈ü√ºnebilirsiniz.\n",
    "\n",
    "**Further Training:**:  Daha Fazla Eƒüitim: Bir√ßok tahmin d√º≈ü√ºk g√ºven g√∂steriyorsa, daha √ße≈üitli verilerle ek eƒüitim, modelin doƒüruluƒüunu ve g√ºvenini artƒ±rmaya yardƒ±mcƒ± olabilir.\n",
    "\n",
    "**Adjust Model Parameters** Model Parametrelerini Ayarlayƒ±n: √ñƒürenme oranƒ± veya yƒ±ƒüƒ±n boyutu gibi hiperparametrelerin ayarlanmasƒ± da tespit doƒüruluƒüunu artƒ±rabilir.\n",
    "\n",
    "**√ñzet** : Saƒülanan g√∂r√ºnt√º, modelin genel olarak doƒürulama g√∂r√ºnt√ºlerindeki insanlarƒ± tespit edebildiƒüini, ancak farklƒ± g√ºven derecelerine sahip olduƒüunu g√∂stermektedir. Bu √ßƒ±ktƒ±, modelin g√∂r√ºnmeyen veriler √ºzerindeki performansƒ±nƒ± deƒüerlendirmek ve modelin daha fazla iyile≈ütirmeye ihtiya√ß duyabileceƒüi alanlarƒ± belirlemek i√ßin kullanƒ±≈ülƒ±dƒ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ODk1VTlevxn"
   },
   "source": [
    "## Validate Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpyuwrNlXc1P",
    "outputId": "6920587d-fd6f-499c-c6b8-4d2974e48531"
   },
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml:**\n",
    "\n",
    "**!yolo**: YOLO komut satƒ±rƒ± aray√ºz√ºn√º √ßaƒüƒ±rƒ±r.\n",
    "\n",
    "**task=detect**: G√∂revin nesne algƒ±lama olduƒüunu belirtir.\n",
    "\n",
    "**mode=val**: Modun doƒürulama olduƒüunu, yani modelin bir doƒürulama veri k√ºmesi √ºzerinde deƒüerlendirileceƒüini belirtir.\n",
    "\n",
    "**model={HOME}/runs/detect/train/weights/best.pt:** Eƒüitim s√ºreci sƒ±rasƒ±nda kaydedilen eƒüitilmi≈ü modelin aƒüƒ±rlƒ±klar dosyasƒ±nƒ±n (best.pt) yolunu belirtir. Bu model doƒürulama i√ßin kullanƒ±lacaktƒ±r.\n",
    "\n",
    "**data={dataset.location}/data.yaml**  Doƒürulama veri k√ºmesi hakkƒ±nda bilgi i√ßeren veri yapƒ±landƒ±rma dosyasƒ±nƒ±n (data.yaml) yolunu belirtir.\n",
    "\n",
    "**Output Analysis:**(Cikti Analizi)\n",
    "\n",
    "**Ultralytics YOLOv8.0.196:** Kullanƒ±lan YOLOv8 s√ºr√ºm√º.\n",
    "\n",
    "**Environment Details:** \n",
    "\n",
    "**Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB):** Bu satƒ±r Python ortamƒ±, PyTorch s√ºr√ºm√º ve doƒürulama i√ßin kullanƒ±lan CUDA √∂zellikli GPU (Tesla T4) hakkƒ±nda ayrƒ±ntƒ±lar saƒülar.\n",
    "\n",
    "**Model Summary:(Model Ozeti)168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs:** \n",
    "\n",
    "168 katman, 11125971 parametreler, 0 gradyan, 28,4 GFLOPs: Bu, katman sayƒ±sƒ±, toplam parametreler ve GFLOP (Giga Kayan Nokta ƒ∞≈ülemleri) cinsinden hesaplama karma≈üƒ±klƒ±ƒüƒ± dahil olmak √ºzere YOLOv8 modelinin mimarisini √∂zetlemektedir.\n",
    "\n",
    "\n",
    "**Validation Process:**(Dogrulama Sureci) :\n",
    "\n",
    "**Scanning /content/datasets/analiz-2/valid/labels.cache** Model, belirtilen yolda bulunan doƒürulama veri k√ºmesini tarƒ±yor.\n",
    "\n",
    "**Images: 3, Instances: 27:**  G√∂r√ºnt√ºler: 3, √ñrnekler: 27: Doƒürulama veri k√ºmesinin 3 g√∂r√ºnt√º ve algƒ±lanacak nesnelerin 27 √∂rneƒüini i√ßerdiƒüini g√∂sterir.\n",
    "\n",
    "**Box(P R):**  Doƒürulama sonu√ßlarƒ± Hassasiyet (P), Geri √áaƒüƒ±rma (R), mAP50 ve mAP50-95 metriklerini i√ßerir:\n",
    "\n",
    "       **Precision (P) ** Kesinlik (P): 0,63; bu da tespit edilen nesnelerin %63'√ºn√ºn ger√ßek pozitif olduƒüunu g√∂stermektedir.\n",
    "       \n",
    "       **Recall (R): ** Hatƒ±rlama (R): 0,63, yani model veri k√ºmesindeki ger√ßek nesnelerin %63'√ºn√º doƒüru bir ≈üekilde tanƒ±mlamƒ±≈ütƒ±r.\n",
    "       \n",
    "       **mAP50: ** 0.488, 0.50'lik Birlik √úzerinde Kesi≈üme (IoU) e≈üiƒüinde ortalama Ortalama Hassasiyeti temsil eder.\n",
    "       \n",
    "       **mAP50-95** 0.145, 0.50 ila 0.95 arasƒ±nda birden fazla IoU e≈üiƒüi arasƒ±nda hesaplanan mAP'dir. Bu metrik, modelin performansƒ±nƒ±n daha kapsamlƒ± bir deƒüerlendirmesini verir.\n",
    "       \n",
    "       \n",
    "       **Speed:** \n",
    "       \n",
    "       **0.4ms preprocess, 29.5ms inference, 0.0ms loss, 201.6ms postprocess per image: ** G√∂r√ºnt√º ba≈üƒ±na 0,4 ms √∂n i≈ülem, 29,5 ms √ßƒ±karƒ±m, 0,0 ms kayƒ±p, 201,6 ms son i≈ülem: Bunlar, √∂n i≈üleme, √ßƒ±karƒ±m ve son i≈üleme dahil olmak √ºzere doƒürulama s√ºrecinin her a≈üamasƒ± i√ßin zamanlamalardƒ±r.\n",
    "       \n",
    "       \n",
    "       **Results:** \n",
    "       \n",
    "       **Results saved to runs/detect/val** Results saved to runs/detect/val: Metrikler ve g√∂rselle≈ütirmeler de dahil olmak √ºzere doƒürulama sonu√ßlarƒ± daha sonra incelenmek √ºzere belirtilen dizine kaydedilir.\n",
    "       \n",
    "       **Documentation:** YOLO'daki doƒürulama modlarƒ± hakkƒ±nda daha fazla bilgi edinmek i√ßin bir baƒülantƒ± verilmi≈ütir: https://docs.ultralytics.com/modes/val.\n",
    "       \n",
    "## √ñzet:\n",
    "\n",
    "Bu diz√ºst√º bilgisayar h√ºcresi, k√º√ß√ºk bir doƒürulama veri k√ºmesi (27 √∂rnekli 3 g√∂r√ºnt√º) kullanarak bir YOLOv8 modelini doƒürulamaktadƒ±r. Modelin performansƒ± hassasiyet, geri √ßaƒüƒ±rma ve mAP deƒüerleri gibi temel metriklerle √∂zetlenmi≈ütir.\n",
    "\n",
    "Sonu√ßlar, mAP deƒüerlerinin g√∂sterdiƒüi gibi orta d√ºzeyde performans ile dengeli bir hassasiyet ve geri √ßaƒüƒ±rma g√∂stermektedir. Bu metrikler, modelin g√∂r√ºlmeyen verilere ne kadar iyi genelleme yaptƒ±ƒüƒ±nƒ± deƒüerlendirmek i√ßin kullanƒ±labilir.\n",
    "\n",
    "S√ºre√ß ayrƒ±ca, modelin ger√ßek zamanlƒ± uygulamalardaki verimliliƒüini deƒüerlendirirken √∂nemli olabilecek hƒ±z √∂l√ß√ºmlerini de kaydeder.\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eASbcWkQBq"
   },
   "source": [
    "## Inference with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wjc1ctZykYuf",
    "outputId": "f880da3d-f821-44a9-cbc4-6d0e0f44d9a2"
   },
   "source": [
    "%cd {HOME}\n",
    "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OZET:** \n",
    "\n",
    "Bu diz√ºst√º bilgisayar h√ºcresi, bir test g√∂r√ºnt√ºs√º √ºzerinde √∂zel bir YOLOv8 modeli kullanarak √ßƒ±karƒ±m yapmaktadƒ±r. Model g√∂r√ºnt√ºdeki 14 insanƒ± ba≈üarƒ±yla tespit etti ve s√ºre√ß bazƒ± √∂n i≈üleme, √ßƒ±karƒ±m ve son i≈üleme adƒ±mlarƒ±nƒ± i√ßeriyordu. Tespit edilen nesneler ve ilgili sƒ±nƒ±rlayƒ±cƒ± kutular da dahil olmak √ºzere sonu√ßlar daha fazla inceleme i√ßin kaydedilmi≈ütir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYIo95n-I0S"
   },
   "source": [
    "**NOTE:** Let's take a look at few results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "jbVjEtPAkz3j",
    "outputId": "089580a8-dd30-49f7-f18d-891478e0e362"
   },
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the base path where the folders are located\n",
    "base_path = '/content/runs/detect/'\n",
    "\n",
    "# List all directories that start with 'predict' in the base path\n",
    "subfolders = [os.path.join(base_path, d) for d in os.listdir(base_path)\n",
    "              if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict')]\n",
    "\n",
    "# Find the latest folder by modification time\n",
    "latest_folder = max(subfolders, key=os.path.getmtime)\n",
    "\n",
    "image_paths = glob.glob(f'{latest_folder}/*.jpg')[:3]\n",
    "\n",
    "# Display each image\n",
    "for image_path in image_paths:\n",
    "    display(Image(filename=image_path, width=600))\n",
    "    print(\"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saƒülanan  g√∂r√ºnt√º, belirli bir dizindeki g√∂r√ºnt√ºleri g√∂r√ºnt√ºlemek i√ßin Python kodunun kullanƒ±ldƒ±ƒüƒ± ve muhtemelen bir nesne algƒ±lama modelinin sonu√ßlarƒ±nƒ± g√∂steren bir Jupyter Notebook h√ºcresini g√∂stermektedir. Kodun altƒ±nda, tespit edilen nesnelerin (insanlar) sƒ±nƒ±rlayƒ±cƒ± kutular ve g√ºven puanlarƒ± ile a√ßƒ±klandƒ±ƒüƒ± bir g√∂r√ºnt√º var.\n",
    "\n",
    "\n",
    "**1-import glob and from IPython.display import Image, display:** \n",
    "\n",
    "**glob** Belirli bir desenle e≈üle≈üen dosyalarƒ± aramanƒ±za olanak tanƒ±yan dosya deseni e≈üle≈ütirmesi i√ßin kullanƒ±lƒ±r.\n",
    "\n",
    "**Image, display** Bunlar, g√∂r√ºnt√ºleri doƒürudan Jupyter Notebook i√ßinde g√∂r√ºnt√ºlemek i√ßin kullanƒ±lƒ±r.\n",
    "\n",
    "\n",
    "**2-Base Path Definition:** \n",
    "\n",
    "**base_path = '/content/runs/detect/'** Modelin √ßƒ±ktƒ± klas√∂rlerinin bulunduƒüu dizini tanƒ±mlar.\n",
    "\n",
    "**3-List Directories:** \n",
    "\n",
    "**subfolders = [os.path.join(base_path, d) for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict')]:**\n",
    "Bu satƒ±r, temel yolda \"predict\" adƒ±yla ba≈ülayan alt klas√∂rlerin bir listesini olu≈üturur.\n",
    "\n",
    "**4-Find the Latest Folder:** \n",
    "\n",
    "**latest_folder = max(subfolders, key=os.path.getmtime)** \"Tahmin\" klas√∂rleri listesinden en son deƒüi≈ütirilen klas√∂r√º bulur.\n",
    "\n",
    "**5-Image Paths:** \n",
    "\n",
    "**image_paths = glob.glob(f'{latest_folder}/*.jpg')[:3]:** En son klas√∂rdeki ilk √º√ß .jpg g√∂r√ºnt√ºs√ºn√ºn yollarƒ±nƒ± alƒ±r.\n",
    "\n",
    "**6-Display Images:**\n",
    "\n",
    "**for image_path in image_paths::**  G√∂r√ºnt√º yollarƒ± listesi √ºzerinde yineleme yapar.\n",
    "\n",
    "**display(Image(filename=image_path, width=600)):**  Not defterindeki her g√∂r√ºnt√ºy√º 600 piksel geni≈üliƒüe √∂l√ßeklendirilmi≈ü olarak g√∂r√ºnt√ºler.\n",
    "\n",
    "**print(\"\\n\")**  Daha iyi okunabilirlik i√ßin g√∂r√ºnt√ºlenen g√∂r√ºnt√ºlerin arasƒ±na yeni satƒ±r ekler.\n",
    "\n",
    "\n",
    "## Image Analysis:\n",
    "\n",
    "Kodun altƒ±nda g√∂r√ºnt√ºlenen g√∂r√ºnt√º, bir sahnede birden fazla insanƒ±n tespit edildiƒüi bir nesne algƒ±lama modelinden elde edilen √ßƒ±ktƒ±yƒ± g√∂stermektedir.\n",
    "\n",
    "**Bounding Boxes** Kƒ±rmƒ±zƒ± dikd√∂rtgenler modelin insanlarƒ± tespit ettiƒüi alanlarƒ± vurgulamaktadƒ±r.\n",
    "\n",
    "**Confidence Scores**(G√ºven Puanlarƒ±)\n",
    "\"ƒ∞nsan\" etiketlerinin yanƒ±ndaki sayƒ±lar (√∂rneƒüin, 0,61, 0,36, 0,56) modelin her bir tespit i√ßin g√ºven d√ºzeyini temsil eder. Daha y√ºksek bir puan, modelin tespit edilen nesnenin ger√ßekten bir insan olduƒüundan daha emin olduƒüu anlamƒ±na gelir.\n",
    "\n",
    "**√ñzet: Bu kod par√ßacƒ±ƒüƒ±, en son nesne algƒ±lama sonu√ßlarƒ±nƒ± bulma ve g√∂r√ºnt√ºleme i≈ülemini otomatikle≈ütirir. G√∂r√ºnt√ºlenen g√∂r√ºnt√º, modelin karedeki birden fazla insanƒ± deƒüi≈üen g√ºven d√ºzeyleriyle ba≈üarƒ±yla tespit ettiƒüini doƒürular.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0tsVilOCPyq"
   },
   "source": [
    "## Deploy model on Roboflow\n",
    "\n",
    "Once you have finished training your YOLOv8 model, you‚Äôll have a set of trained weights ready for use. These weights will be in the `/runs/detect/train/weights/best.pt` folder of your project. You can upload your model weights to Roboflow Deploy to use your trained weights on our infinitely scalable infrastructure.\n",
    "\n",
    "The `.deploy()` function in the [Roboflow pip package](https://docs.roboflow.com/python) now supports uploading YOLOv8 weights.\n",
    "\n",
    "To upload model weights, add the following code to the ‚ÄúInference with Custom Model‚Äù section in the aforementioned notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:53:42.840915Z",
     "start_time": "2024-08-13T15:53:41.658580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EhBAJ2gCPZh",
    "outputId": "0c064d40-7a1e-4697-fcab-afab40ba50b4"
   },
   "source": [
    "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5kOhjkmcV1l"
   },
   "source": [
    "#While your deployment is processing, checkout the deployment docs to take your model to most destinations https://docs.roboflow.com/inference"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4bpUIibcV1l"
   },
   "source": [
    "#Run inference on your model on a persistant, auto-scaling, cloud API\n",
    "\n",
    "#load model\n",
    "model = project.version(dataset.version).model\n",
    "\n",
    "#choose random test set image\n",
    "import os, random\n",
    "test_set_loc = dataset.location + \"/test/images/\"\n",
    "random_test_image = random.choice(os.listdir(test_set_loc))\n",
    "print(\"running inference on \" + random_test_image)\n",
    "\n",
    "pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n",
    "pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii6oacluX2Fn"
   },
   "source": [
    "# Deploy Your Model to the Edge\n",
    "\n",
    "In addition to using the Roboflow hosted API for deployment, you can use [Roboflow Inference](https://inference.roboflow.com), an open source inference solution that has powered millions of API calls in production environments. Inference works with CPU and GPU, giving you immediate access to a range of devices, from the NVIDIA Jetson to TRT-compatible devices to ARM CPU devices.\n",
    "\n",
    "With Roboflow Inference, you can self-host and deploy your model on-device. You can deploy applications using the [Inference Docker containers](https://inference.roboflow.com/quickstart/docker/) or the pip package.\n",
    "\n",
    "For example, to install Inference on a device with an NVIDIA GPU, we can use:\n",
    "\n",
    "```\n",
    "docker pull roboflow/roboflow-inference-server-gpu\n",
    "```\n",
    "\n",
    "Then we can run inference via HTTP:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "workspace_id = \"\"\n",
    "model_id = \"\"\n",
    "image_url = \"\"\n",
    "confidence = 0.75\n",
    "api_key = \"\"\n",
    "\n",
    "infer_payload = {\n",
    "    \"image\": {\n",
    "        \"type\": \"url\",\n",
    "        \"value\": image_url,\n",
    "    },\n",
    "    \"confidence\": confidence,\n",
    "    \"iou_threshold\": iou_thresh,\n",
    "    \"api_key\": api_key,\n",
    "}\n",
    "res = requests.post(\n",
    "    f\"http://localhost:9001/{workspace_id}/{model_id}\",\n",
    "    json=infer_object_detection_payload,\n",
    ")\n",
    "\n",
    "predictions = res.json()\n",
    "```\n",
    "\n",
    "Above, set your Roboflow workspace ID, model ID, and API key.\n",
    "\n",
    "- [Find your workspace and model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids?ref=blog.roboflow.com)\n",
    "- [Find your API key](https://docs.roboflow.com/api-reference/authentication?ref=blog.roboflow.com#retrieve-an-api-key)\n",
    "\n",
    "Also, set the URL of an image on which you want to run inference. This can be a local file.\n",
    "\n",
    "_To use your YOLOv5 model commercially with Inference, you will need a Roboflow Enterprise license, through which you gain a pass-through license for using YOLOv5. An enterprise license also grants you access to features like advanced device management, multi-model containers, auto-batch inference, and more._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP1kp4mjX2Fn"
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQgOj_xSNDg"
   },
   "source": [
    "## üèÜ Congratulations\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
    "\n",
    "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
    "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
    "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
    "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
    "\n",
    "### Convert data formats\n",
    "\n",
    "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
    "\n",
    "### Connect computer vision to your project logic\n",
    "\n",
    "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
